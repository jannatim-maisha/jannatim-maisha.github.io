---
layout: archive
title: "Research"
permalink: /research/
author_profile: true
---
<b>Areas of Interests:</b>
Machine Learning, Natural Language Processing, Computer Vision, Large Language Model, Signal Processing.
  
## Publications
<script src="https://bibbase.org/show?bib=https%3A%2F%2Fbibbase.org%2Fnetwork%2Ffiles%2F8DrJ4qd2uE8fmm5JR&noBootstrap=1&jsonp=1"></script>

## Under Review
<ul><li>F. H. Swarnali, <a style="text-decoration: none">J. Maisha</a>, M. A. Mahtab, M. S. I. Iftikar, and F. M. Shah, “**Bengali multi-class text classification via enhanced contrastive learning techniques**,” in 2024 27th International Conference on Computer and Information Technology (ICCIT), Cox’s Bazar, Bangladesh.<br/>
<!-- <span style="font-style: italic"><font  size="2">- Extending thesis work by introducing new loss functions, Token-level Adversarial Contrastive Training (TACT)
and Label-aware Contrastive Loss (LCL), to improve text classification in Bengali. Developed a
custom multi-class dataset for text classification. Applied contrastive learning techniques using a pre-trained
Bengali BERT model, achieving new benchmarks for binary and multi-class sentiment analysis.</font></span><br> -->
<details> <summary>Abstract</summary> <span style="text-align:justify; display:block;"> <font size="3"> Bengali, one of South Asia's most frequently spoken languages, poses substantial difficulties in tasks such as sentiment analysis and other forms of text classification due to its intricate grammatical structure. This is not just vital for protecting mental health through precise sentiment analysis, but it also has broader ramifications in sectors where accurately discriminating between fine-grained meanings is critical. Improving classification methods to address these subtle distinctions is a timely necessity for advancing natural language processing in Bengali. Our study aims to advance the field of Bengali text classification by implementing Token-level Adversarial Contrastive Training (TACT) and Label-aware Contrastive (LCL) loss, leveraging contrastive learning methods. The two new losses distinguished fine-grained text better, compared to our previous findings on Contrastive Adversarial Training (CAT) and Supervised Contrastive Loss (SCL). For binary class classification, TACT reached an F1-score of 98% outperforming CAT and LCL and setting a new benchmark on the Rokomari Book Review (RBR) dataset. For multi-class classification, TACT achieved an F1-score of 91%, matching the current benchmark on the Bengali Hate Speech (BHS-M) dataset. Furthermore, our custom Bengali multi-class text classification dataset, Daraz Product Review (DPR) further contributes to the field. </font> </span><br> </details></ul>

<ul><li> M. A. Mahtab, <a style="text-decoration: none">J. Maisha,</a> M. M. Rahman, and S. K. S. Joy, “**An empirical study on utilizing large language models for bengali image caption generation**,” in 2024 27th International Conference on Computer and Information Technology (ICCIT), Cox’s Bazar, Bangladesh.
<!-- <br/><span style="font-style: italic"><font  size="2">- This study pioneers the use of Large Language Models (LLMs) like BanglaGPT for Bengali image captioning, combining CLIP encodings and Vision Transformer-based architectures. The models significantly outperform existing benchmarks on the BanglaLekha and BNature datasets, achieving high BLEU and CIDEr scores, thus advancing the field of Bengali image captioning.</font></span><br> -->
<br></li><details> <summary>Abstract</summary> <span style="text-align:justify; display:block;"> <font size="3"> An exemplary caption not only describes what is happening in a particular image but also denotes intricate traditional objects in the image by their local representative terms through which the native speakers can recognize the object in question. A caption that fails to accomplish the latter is not effective in conveying proper utility. To ensure caption locality, we aim to explore the potential of Large Language Models (LLM) in Bengali image captioning, which have lately shown promising results in English language caption generation. As a first for the Bengali language, we utilized CLIP (Contrastive Language-Image Pre-training) encodings as a prefix to the captions by employing a mapping network, followed by fine-tuning BanglaGPT, a Bengali pre-trained large language model to generate the image captions. Furthermore, we explored vision transformer-based encoders (ViT, Swin) with BanglaGPT as the decoder. The best BanglaGPT-based model outperformed the current benchmark results, with BLEU-1, BLEU-2, BLEU-3, BLEU-4, METEOR, and CIDEr scores of 70.2, 63.9, 58.8, 54.3, 39.2, and 95.9 on the BanglaLekha dataset and 82.4, 76.8, 71.9, 67.4, 36.6, and 76.9 on the BNature dataset. </font> </span> </details></ul>

## Conference Presentation
* **A Study of Contrastive Learning Methods for Bengali Social Analysis**, 6th International Conference on Electrical Engineering and Information & Communication Technology (ICEEICT), Dhaka, Bangladesh. <a href="https://www.youtube.com/watch?v=Czj9QxdQjM">Presentation link</a>

## Ongoing Projects
* Contrastive Learning methods in code-mixed languages.
* Sub-Dialect Detection and Translation.
* Plagiarism Detection in Bengali Cover Songs.



__________________________________________________